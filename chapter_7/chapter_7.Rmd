---
title: "ISLR | Chapter 7 Exercises"
author: "Marshall McQuillen"
date: "9/21/2018"
output: 
  pdf_document: 
    latex_engine: xelatex
---

# Conceptual

## 1

* **A**. The cubic piecewise polynomial:

$$
f(x) = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 + \beta_4(x - \xi)^3_+ ~~~ 
where ~~~ (x - \xi)^3_+ = 
\begin{cases}
0, ~~~ x \le \xi \\ 
(x - \xi)^3, ~~~ otherwise
\end{cases}
$$

\setlength{\leftskip}{1cm}

...can be broken up and rewritten to be:

$$
f(x) = 
\begin{cases}
f_1(x) = a_1 + b_1x + c_1x^2 + d_1x^3, ~~~ x \le \xi \\ 
f_2(x) = a_2 + b_2x + c_2x^2 + d_2x^3, ~~~ otherwise
\end{cases}
$$

In $f_1(x)$, since $(x - \xi)^3_+ = 0$ (because $x \le \xi$), the fifth term (of $f(x)$) zeroes out and the coefficients can be expresses as $a_1 = \beta_0, ~b_1 = \beta_1, ~c_1 = \beta_2$ and $d_1 = \beta_3$.

\setlength{\leftskip}{0cm}

\hfill

* **B**. Expanding the fifth term in $f(x)$ allows for the various powers of $x$ to be grouped together and then recondensed. $a_2, ~b_2, ~c_2$ and $d_2$ are expressed in terms of the cofficients below.

\begin{align}
f_2(x) & = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 + \beta_4(x - \xi)^3 \\
& = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 + \beta_4(x - \xi)(x - \xi)(x - \xi) \\
& = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 + \beta_4(x^2 - 2x\xi + \xi^2)(x - \xi) \\
& = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 + \beta_4(x^3 -x^2\xi - 2x^2\xi + 2x\xi^2 + \xi^2x - \xi^3) \\
& = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 + \beta_4(x^3 -3x^2\xi + 3x\xi^2 - \xi^3) \\
& = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 + \beta_4x^3 -\beta_43x^2\xi +\beta_4 3x\xi^2 - \beta_4\xi^3 \\
& = (\beta_0  - \beta_4\xi^3) + (\beta_1x + \beta_4 3x\xi^2) + (\beta_2x^2 - \beta_43x^2\xi) + (\beta_3x^3 + \beta_4x^3) \\
& = (\beta_0 - \beta_4\xi^3) + (\beta_1 + 3\beta_4\xi^2)x + (\beta_2 - 3\beta_4\xi)x^2 + (\beta_3 + \beta_4)x^3 \\
f_2(x) & = a_2 + b_2x + c_2x^2 + d_2x^3 ~~~ where ~~~ \begin{cases}
                                                    a_2 = \beta_0 - \beta_4\xi^3 \\
                                                    b_2 = \beta_1 + 3\beta_4\xi^2 \\
                                                    c_2 = \beta_2 - 3\beta_4\xi \\
                                                    d_2 = \beta_3 + \beta_4
                                                    \end{cases}
\end{align}

\pagebreak

* **C**. Showing that $f(x)$ is continuous at $\xi$ is illustrated by showing that $f(\xi)_1 = f(\xi)_2$.

\begin{align}
f_1(\xi) & = a_1 + b_1(\xi) + c_1(\xi)^2 + d_1(\xi)^3 \\
& = \beta_0 + \beta_1(\xi) + \beta_2(\xi)^2 + \beta_3(\xi)^3 \\
\\
f_2(\xi) & = a_2 + b_2(\xi) + c_2(\xi)^2 + d_2(\xi)^3 \\
& = (\beta_0 - \beta_4\xi^3) + (\beta_1 + 3\beta_4\xi^2)(\xi) + (\beta_2 - 3\beta_4\xi)(\xi)^2 + (\beta_3 + \beta_4)(\xi)^3 \\
& = (\beta_0 - \beta_4\xi^3) + (\beta_1\xi + 3\beta_4\xi^3) + (\beta_2\xi^2 - 3\beta_4\xi^3) + (\beta_3\xi^3 + \beta_4\xi^3) \\
& = \beta_0 - \beta_4\xi^3 + \beta_1\xi + 3\beta_4\xi^3 + \beta_2\xi^2 - 3\beta_4\xi^3 + \beta_3\xi^3 + \beta_4\xi^3 \\
& = \beta_0 + \beta_1\xi + \beta_2\xi^2 + \beta_3\xi^3 + 3\beta_4\xi^3 - 3\beta_4\xi^3 + \beta_4\xi^3 - \beta_4\xi^3 \\
& = \beta_0 + \beta_1\xi + \beta_2\xi^2 + \beta_3\xi^3 + (3\beta_4\xi^3 - 3\beta_4\xi^3) + (\beta_4\xi^3 - \beta_4\xi^3) \\
f_2(\xi) & = \beta_0 + \beta_1\xi + \beta_2\xi^2 + \beta_3\xi^3
\end{align}

$$
f_2(\xi) = \beta_0 + \beta_1\xi + \beta_2\xi^2 + \beta_3\xi^3 = f_1(\xi)
$$

\hfill

* **D**. In order to show that $f'_1(\xi) = f'_2(\xi)$, we must first find $f'(x)$ with respect to $x$ and then simplify both $f'_1(\xi)$ and $f'_2(\xi)$.

\begin{align}
f(x) & = a_1 + b_1x + c_1x^2 + d_1x^3 \\
f'(x) & = b_1 + 2c_1x + 3d_1x^2
\end{align}

\setlength{\leftskip}{1cm}

Therefore, substituting the necessary coefficients in for $b_1, ~c_1$ and $d_1$ in both $f'_1(\xi)$ and $f'_2(\xi)$, we get:

\begin{align}
f'(x) & = b_1 + 2c_1x + 3d_1x^2 ~~~ then ~~~ \begin{cases}
                                            f'_1(\xi) = \beta_1 + 2\beta_2\xi + 3\beta_3\xi^2 \\
                                            f'_2(\xi) = (\beta_1 + 3\beta_4\xi^2) + 2(\beta_2 - 3\beta_4\xi )\xi + 3(\beta_3 + \beta_4)\xi^2
                                              \end{cases}
\end{align}

\begin{align}
f'_2(\xi) & = (\beta_1 + 3\beta_4\xi^2) + 2(\beta_2 - 3\beta_4\xi )\xi + 3(\beta_3 + \beta_4)\xi^2 \\
& = \beta_1 + 3\beta_4\xi^2 + 2\beta_2\xi - 6\beta_4\xi^2 + 3\beta_3\xi^2 + 3\beta_4\xi^2 \\
& = \beta_1 + 2\beta_2\xi + 3\beta_3\xi^2 + (3\beta_4\xi^2 + 3\beta_4\xi^2 - 6\beta_4\xi^2) \\
& = \beta_1 + 2\beta_2\xi + 3\beta_3\xi^2 + (6\beta_4\xi^2 - 6\beta_4\xi^2) \\
f'_2(\xi) & = \beta_1 + 2\beta_2\xi + 3\beta_3\xi^2
\end{align}


We now see that the derivative $f'(x)$ is continuous at knot $\xi$, which is to say $f'_1(\xi) = f'_2(\xi)$:

$$
f'_2(\xi) = \beta_1 + 2\beta_2\xi + 3\beta_3\xi^2 = f'_1(\xi)
$$

\setlength{\leftskip}{0cm}

\pagebreak

* **E**. In order to show that $f''_1(\xi) = f''_2(\xi)$, we must first find $f''(x)$ with respect to $x$ and then simplify both $f''_1(\xi)$ and $f''_2(\xi)$.

\begin{align}
f(x) & = a_1 + b_1x + c_1x^2 + d_1x^3 \\
f'(x) & = b_1 + 2c_1x + 3d_1x^2 \\
f''(x) & = 2c_1 + 6d_1x
\end{align}

\setlength{\leftskip}{1cm}

Therefore, substituting the necessary coefficients in for $c_1$ and $d_1$ in both $f''_1(\xi)$ and $f''_2(\xi)$, we come to:

\begin{align}
f''(x) & = 2c_1 + 6d_1x ~~~ then ~~~ \begin{cases}
                                    f''_1(\xi) = 2\beta_2 + 6\beta_3\xi \\
                                    f''_2(\xi) = 2(\beta_2 - 3\beta_4\xi) + 6(\beta_3 + \beta_4)\xi
                                     \end{cases}
\end{align}

\begin{align}
f''_2(\xi) & = 2(\beta_2 - 3\beta_4\xi) + 6(\beta_3 + \beta_4)\xi \\
& = 2\beta_2 - 6\beta_4\xi + 6\beta_3\xi + 6\beta_4\xi \\
& = 2\beta_2 + 6\beta_3\xi + (6\beta_4\xi - 6\beta_4\xi) \\
f''_2(\xi) & = 2\beta_2 + 6\beta_3\xi
\end{align}

We now see that the second derivative $f''(x)$ is continuous at knot $\xi$, which is to say $f''_1(\xi) = f''_2(\xi)$:

$$
f''_2(\xi) = 2\beta_2 + 6\beta_3\xi = f''_1(\xi)
$$

\setlength{\leftskip}{0cm}

## 2

(sketches on following page)

* **A**. With $\lambda = \infty$, the second term will dominate the above equation and the RSS will be ignored. Since $g^0 = g$, this comes out to finding $g(x)$ that minimizes the integral of $g(x)$. Therefore, $g(x) = 0$.

* **B**. With $\lambda = \infty$ and $m = 1$, the second term will dominate the above equation and the RSS will be ignored. This then becomes a problem of finding a function $g(x)$ where $\int g'(x)$ is minimized. Therefore, $g(x) = c$ (a flat line) where $c$ is a constant, ensuring that $g'(x) = 0$.

* **C**. With $\lambda = \infty$ and $m = 2$, the second term will dominate the above equation and the RSS will be ignored. This then becomes a problem of finding a function $g(x)$ where $\int g''(x)$ is minimized.

\setlength{\leftskip}{1cm}

If we work backwards conceptually, we will see that $g(x) = \beta_0 + \beta_1 x$. Since $\int g''(x)$ must be minimized, $g''(x) = 0$. Therefore, $g'(x) = c$ where $c$ is some constant. This implies that $g(x)$ must have a constant slope, $c$ aka $\beta_1$. Therefore, $g(x) = \beta_0 + \beta_1 x$

\setlength{\leftskip}{0cm}

* **D**. With $\lambda = \infty$ and $m = 3$, the second term will dominate the above equation and the RSS will be ignored. This then becomes a problem of finding a function $g(x)$ where $\int g'''(x)$ is minimized. Therefore, $g(x) = \beta_0 + \beta_1x + \beta_2x^2$, $g(x)$ will be quadratic in some sense

\setlength{\leftskip}{1cm}

Once again, working backwards conceptually, if the goal is to minimize $\int g'''(x)$, then $g'''(x) = 0$. Therefore, $g''(x) = c$, where $c$ is some constant. This implies that $g'(x)$ must have a constant slope, $c$. if $g'(x)$ has a constant slope, then $g(x) = \beta_0 + \beta_1x + \beta_2x^2$. Having a quadratic equation means that the slope of $g(x)$ is changing at a fixed rate, which satisfies our condition that $g'(x) = c$. 

\setlength{\leftskip}{0cm}

* **E**. With $\lambda = 0$ and $m = 3$, the second term in the equation is completely ignored, and $g(x)$ becomes the line that interpolates all data points.

!["Conceptual Exercise 2"](images/conceptual_2.jpg)

## 3

$$
f(x) = 1 + x +  \begin{cases}
                -2(x - 1)^2,~ x \ge 1 \\
                0, ~otherwise
                \end{cases}
$$

The intercept is at $y = 1$, $f(x)$ is linear with a slope equal to 1 up to $x = 1$, after which it becomes quadratic.

\hfill

```{r, echo=FALSE}
x <- -2:2
y <- 1 + x + (-2*(x-1)^2 * I(x >= 1))
plot(x, y, type = 'l', main = "f(x)")
points(0,1, pch = 16)
text(0.5,1,labels = 'Intercept (0, 1)')
```

\pagebreak

## 4

\begin{align}
f(x) & = \beta_0 + \beta_1b_1(x) + \beta_2b_2(x) \\
f(x) & = 1 + b_1(x) + 3b_2(x) ~~~ where ~~~
\begin{cases}
    b_1(x) = I(0 \leq x \leq 2) - (x - 1)I(1 \leq x \leq 2) \\
    b_2(x) = (x - 3)I(3 \leq x \leq 4) + I(4 < x \leq 5)
\end{cases}
\end{align}

```{r}
x <- -2:2
y <- c(1,1,2,2,1)
plot(x, y)
```

## 5

\begin{align}
\hat{g_1} & = \left( \sum_{i=1}^n (y_i - g(x_i))^2 + \int[g^3(x)]^2dx \right) \\
\hat{g_2} & = \left( \sum_{i=1}^n (y_i - g(x_i))^2 + \int[g^4(x)]^2dx \right)
\end{align}

* **A**. As $\lambda \to \infty$, $\hat{g_2}$ will have a smaller training RSS. This is because $\hat{g_2}$ has one more degree of freedom than $\hat{g_1}$; in other words, it is allowed to be more flexible thatn $\hat{g_1}$.

* **B**. As $\lambda \to \infty$, $\hat{g_1}$ will most likely have a lower test RSS, although this is less certain than part **A**. It will most likely have a lower test RSS because we are constraining it more, which is to say there is less of a chance that it incorporates the error term $\epsilon$ into the model itself.

* **C**. If $\lambda = 0$, the two equations are the same so they will have the same training and test RSS (one that interpolates all data points).

# Applied

## 6

* **A**. Using 10-Fold CV of wage predicted by age for polynomial fits ranging in degree from 1 to 10, the minimus MSE is at a degree of 10. However, the RMSE only improves marginally after a third degree polynomial. Therefore, since a more complex model is only justifiable when accompanied by a significant decrease in the error rate, I will move forward with the third degree polynomial (which coincides with the results obtained from ANOVA.

```{r}
# imports
suppressPackageStartupMessages(library(ISLR))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(boot))
suppressPackageStartupMessages(library(ggplot2))
attach(Wage)

set.seed(5)

# 10-Fold Polynomial models with degree 1 - 10
degrees <- 1:10
cv.errors <- rep(0, 10)
for (i in degrees) {
    cv.fit <- glm(wage ~ poly(age, i), data = Wage)
    cv.errors[i] <- cv.glm(Wage, cv.fit, K = 10)$delta[1]
}

# Plot of CV errors
g <- ggplot(data.frame(x=1:10, y=sqrt(cv.errors)), aes(x, y)) +
    geom_point() +
    geom_point(aes(x=which.min(cv.errors),
                   y=sqrt(cv.errors[which.min(cv.errors)])),
               color = 'firebrick1',
               shape = "O",
               size = 6) +
     geom_point(aes(x=3,
               y=sqrt(cv.errors[3])),
           color = 'royalblue1',
           shape = "O",
           size = 6) +
    scale_x_continuous(breaks = 1:10,
                     labels = as.character(c(1:10))) +   
    ggtitle("Average RMSE Over 10-Fold Cross Validation") +
    xlab("Degree") +
    ylab("Average RMSE")
g

# ANOVA
fit.1 <- lm(wage ~ age, data = Wage)
fit.2 <- lm(wage ~ poly(age, 2), data = Wage)
fit.3 <- lm(wage ~ poly(age, 3), data = Wage)
fit.4 <- lm(wage ~ poly(age, 4), data = Wage)
fit.5 <- lm(wage ~ poly(age, 5), data = Wage)
fit.6 <- lm(wage ~ poly(age, 6), data = Wage)
fit.7 <- lm(wage ~ poly(age, 7), data = Wage)
fit.8 <- lm(wage ~ poly(age, 8), data = Wage)
fit.9 <- lm(wage ~ poly(age, 9), data = Wage)
fit.10 <- lm(wage ~ poly(age, 10), data = Wage)
anova(fit.1, fit.2, fit.3, fit.4, fit.5, fit.6, fit.7, fit.8, fit.9, fit.10)

# Plot 3rd degree polynomial
g <- ggplot(Wage,
            aes(x = age, y = wage)) +
     geom_point(color = 'lightblue') +
     stat_smooth(method = 'lm',
                 formula = y ~ poly(x, 3),
                 size = 1,
                 color = 'red') +
     ggtitle("3rd Degree Polynomial of Wage ~ Age") +
     xlab("Age") +
     ylab("Wage")
g
```