---
title: "ISLR | Chapter 10 Exercises"
author: "Marshall McQuillen"
date: "2/8/2019"
output: 
  pdf_document: 
    latex_engine: xelatex
urlcolor: blue
---

# Conceptual

## 1

**NEED TO COME BACK TOO**

* **A**. 10.12, illustrated below, is showing the the *within-cluster variation* is equal to twice the squared distance between each data point in cluster *k* ($C_k$) and that cluster's centroid, summed across all data points.


\begin{align}
\frac{1}{\lvert C_k \rvert}\sum_{i,i' \in C_k}\sum_{j=1}^P (x_{i,j} - x_{i',j})^2 & = 2\sum_{i \in C_k}\sum_{j=1}^P (x_{i,j} - \bar{x}_{k,j})^2 \\
\frac{1}{\lvert C_k \rvert}\sum_{i,i' \in C_k}\sum_{j=1}^P (x_{i,j} - x_{i',j})(x_{i,j} - x_{i',j}) & = 2\sum_{i \in C_k}\sum_{j=1}^P (x_{i,j} - \bar{x}_{k,j})(x_{i,j} - \bar{x}_{k,j}) \\
\frac{1}{\lvert C_k \rvert}\sum_{i,i' \in C_k}\sum_{j=1}^P (x_{i,j}^2 - 2x_{i,j}x_{i',j} + x_{i',j}^2) & = 2\sum_{i \in C_k}\sum_{j=1}^P (x_{i,j}^2 - 2\bar{x}_{k,j}x_{i,j} + \bar{x}_{k,j}^2) \\
\frac{\lvert C_k \rvert}{\lvert C_k \rvert}\sum_{i,i' \in C_k}\sum_{j=1}^P (x_{i,j}^2 - 2x_{i,j}x_{i',j} + x_{i',j}^2) & = 2\lvert C_k \rvert\sum_{i \in C_k}\sum_{j=1}^P (x_{i,j}^2 - 2\bar{x}_{k,j}x_{i,j} + \bar{x}_{k,j}^2) \\
\sum_{i,i' \in C_k}\sum_{j=1}^P (x_{i,j}^2 - 2x_{i,j}x_{i',j} + x_{i',j}^2) & = 2\lvert C_k \rvert\sum_{i \in C_k}\sum_{j=1}^P (x_{i,j}^2 - 2\bar{x}_{k,j}x_{i,j} + \bar{x}_{k,j}^2) \\
\sum_{i \in C_k}\sum_{j=1}^P x_{i,j}^2 - 2\sum_{i,i' \in C_k}\sum_{j=1}^Px_{i,j}x_{i',j} + \sum_{i' \in C_k}\sum_{j=1}^P x_{i',j}^2 & = 2\lvert C_k \rvert\sum_{i \in C_k}\sum_{j=1}^P x_{i,j}^2 - 2\lvert C_k \rvert\sum_{i \in C_k}\sum_{j=1}^P2\bar{x}_{k,j}x_{i,j} + 2\lvert C_k \rvert\sum_{i \in C_k}\sum_{j=1}^P\bar{x}_{k,j}^2 \\
\sum_{i \in C_k}\sum_{j=1}^P x_{i,j}^2 - 2\sum_{i,i' \in C_k}\sum_{j=1}^Px_{i,j}x_{i',j} + \sum_{i' \in C_k}\sum_{j=1}^P x_{i',j}^2 & = 2\lvert C_k \rvert\sum_{i \in C_k}\sum_{j=1}^P x_{i,j}^2 - 4\lvert C_k \rvert\sum_{i \in C_k}\sum_{j=1}^P\bar{x}_{k,j}x_{i,j} + 2\lvert C_k \rvert\sum_{i \in C_k}\sum_{j=1}^P\bar{x}_{k,j}^2 \\
\end{align}


## 2

* **A**.

\hfill

```{r}
x <- matrix(c(0, 0.3, 0.4, 0.7,
              0.3, 0, 0.5, 0.8,
              0.4, 0.5, 0, 0.45,
              0.7, 0.8, 0.45, 0), ncol = 4, nrow = 4)

plot(hclust(as.dist(x), method = 'complete'),
     xlab = "",
     sub = "",
     ylab = "Fusion Height")
```

\hfill

* **B**.

\hfill

```{r}
plot(hclust(as.dist(x), method = 'single'),
     xlab = "",
     sub = "",
     ylab = "Fusion Height")
```

\hfill

* **C**. Observations 1 & 2 will be in cluster $A$ and observations 3 & 4 will be in cluster $B$ (assuming one cuts the dendrogram at a height greater than 0.45).

\hfill

* **D**. Although the answer to this question depends on where one cuts the dendrogram, the most likely clusters would contain observations 1 & 2 in cluster $A$ and observations 3 & 4 in cluster $B$. This would results from a cut at a height greater than 0.3 and less than 0.4, which is the largest vertical distance on the dendrogram. If one were to make a cut between 0.4 and 0.45, then cluster $A$ would contain observations 1, 2 & 3, while cluster $B$ would consist of only observation 4. However, with the distance being greater between cluster's for the first grouping, that would be the more probable grouping.

\hfill

* **E**. As shown below, one can simply switch the labels of the observations within each cluster to change the dendrogram without changing the meaning of the dendrogram. In addition, one could take the mirror image of the plot displayed along the dotted red line, producing a "new" dendrogram that has the same meaning.

\hfill

```{r}
plot(hclust(as.dist(x), method = 'complete'),
     xlab = "",
     sub = "",
     ylab = "Fusion Height", labels = c(2,1,4,3))
abline(v = 2.5, col = 'red', lty = 2)
```

\hfill

## 3

* **A**.

\hfill

```{r}
suppressPackageStartupMessages(library(ggplot2))
df <- data.frame(x1 = c(1,1,0,5,6,4),
                 x2 = c(4,3,4,1,2,0))
qplot(df$x1, df$x2, xlab = 'X1', ylab = 'X2')
```

\hfill

* **B**.

\hfill

```{r}
suppressPackageStartupMessages(library(knitr))
df$group <- sample(c('A','B'), 6, replace = TRUE)
knitr::kable(df, caption = 'Sample Data with Group Assignments')
```

\hfill

* **C**.

\hfill

```{r}
group.a <- subset(df, group == 'A')
group.b <- subset(df, group == 'B')

# centroid calculation
calculate.centroid <- function(df, variables=c('x1','x2')) {
    
    coordinates <- NULL
    
    for (variable in variables) {
        column.mean <- mean(df[, variable])
        coordinates <- c(column.mean, coordinates)
    }
    
    return(coordinates)
}
group.a.centroid <- calculate.centroid(group.a)
group.b.centroid <- calculate.centroid(group.b)
print(paste("Group A centroid at coordinates",
            round(group.a.centroid[1], 2), 'and', round(group.a.centroid[2], 2)))
print(paste("Group B centroid at coordinates",
            round(group.b.centroid[1], 2), 'and', round(group.b.centroid[2], 2)))
```

\hfill

* **D**.

\hfill

```{r}
centroid.matrix <- matrix(c(group.a.centroid, group.b.centroid),
                            nrow = 2, 
                            ncol = 2)

reassign.cluster <- function(df,
                             col.idx=c(1,2),
                             cluster.column='group',
                             cluster.labels=c('A','B'),
                             centroids=centroid.matrix) {
    
    df.matrix <- as.matrix(df[, col.idx])

    updated.labels <- NULL
    for (i in 1:dim(df.matrix)[1]) {

        sqr.manhattan.dist <- (df.matrix[i, ] - centroids)^2
        euclidean.dist <- sqrt(colSums(sqr.manhattan.dist))
        closest.centroid <- which.min(euclidean.dist)
        updated.labels <- c(updated.labels, cluster.labels[closest.centroid])

    }
    
    return(data.frame(df.matrix, group = updated.labels))

}
updated.df <- reassign.cluster(df)
knitr::kable(updated.df, caption = "Sample Data After One Reassignment Iteration")
```

\hfill

* **E**.

\hfill

```{r}
for (i in 1:20) {
    # split data
    group.a <- subset(df, group == 'A')
    group.b <- subset(df, group == 'B')
    
    # recalculate centroid
    group.a.centroid <- calculate.centroid(group.a)
    group.b.centroid <- calculate.centroid(group.b)
    
    # create centroid matrix
    centroid.matrix <- matrix(c(group.a.centroid, group.b.centroid), nrow = 2, ncol = 2)
    
    # re-assign group labels
    df <- reassign.cluster(df)
}
```

\hfill

* **F**.

\hfill

```{r}
ggplot(df, aes(x = x1, y = x2, color = group)) +
    geom_point() +
    xlab('X1') +
    ylab('X2') +
    ggtitle('Cluster Assignments after 20 Iterations')
```

\hfill

## 4

\hfill

* **A**. *Assuming the maximal intercluster dissimilarity is* **not equal** *to the minimal intercluster dissimilarity*, the clusters {1, 2, 3} and {4, 5} will be fused higher on the dendrogrogram when complete linkage is used. While complete linkage uses the *maximal* intercluster dissimilarity, single linkage uses the *minimal* intercluster dissimilarity. So, if the first assumption is held, using complete linkage will lead to the fusion occurring higher on the dendrogram. However, if the maximal intercluster dissimilarity is equal to the minimal intercluster dissimilarity, then they would fuse at the same height, regardless of the linkage method used.

* **B**. Assuming that in both scenarios the clusters {5} and {6} are by themselves (i.e. not joined with any other clusters/observations), then the clusters would be joined at the same height regardless of whether single or complete linkage were to be used. As stated in part **A**, complete linkage uses the maximal intercluster dissimilarity while single linkage uses the minimal intercluster dissimilarity. In this scenario, both "clusters" only have one observation in them so the maximal and minimal intercluster dissimilarity will be the exact same. 

# Resources

[Dendrograms in R](http://www.sthda.com/english/wiki/beautiful-dendrogram-visualizations-in-r-5-must-known-methods-unsupervised-machine-learning)